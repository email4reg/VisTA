{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "preprocess-twitter.py\n",
    "python preprocess-twitter.py \"Some random text with #hashtags, @mentions and http://t.co/kdjfkdjf (links). :)\"\n",
    "Script for preprocessing tweets by Romain Paulus\n",
    "with small modifications by Jeffrey Pennington\n",
    "with translation to Python by Motoki Wu\n",
    "Translation of Ruby script to create features for GloVe vectors for Twitter data.\n",
    "http://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import regex as re\n",
    "\n",
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "def hashtag(text):\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = \"<hashtag> {} <allcaps>\".format(hashtag_body)\n",
    "    else:\n",
    "        result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
    "    return result\n",
    "\n",
    "def allcaps(text):\n",
    "    text = text.group()\n",
    "    return text.lower() + \" <allcaps>\"\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"#\\S+\", hashtag)\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "\n",
    "    ## -- I just don't understand why the Ruby script adds <allcaps> to everything so I limited the selection.\n",
    "    # text = re_sub(r\"([^a-z0-9()<>'`\\-]){2,}\", allcaps)\n",
    "    text = re_sub(r\"([A-Z]){2,}\", allcaps)\n",
    "\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"I TEST alllll kinds of #hashtags and #HASHTAGS, @mentions and 3000 (http://t.co/dkfjkdf). w/ <3 :) haha!!!!!\"\n",
    "preproc_text=tokenize(text)\n",
    "\n",
    "print(preproc_text)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "stop += ['<hashtag>', '<url>', '<allcaps>', '<number>', '<user>', '<repeat>', '<elong>', 'websummit','http','https']\n",
    "#stop+=['\\,','.','!','-','/','\\','~',':',';','|','$','%','_']\n",
    "\n",
    "#for tweet in tweets:\n",
    "    #parts = tknzr.tokenize(tweet_processor.preprocess(tweet[\"text\"]))\n",
    "parts = tknzr.tokenize(preproc_text)\n",
    "clean = [i for i in parts if i not in stop]\n",
    "    #tweet[\"processed\"] = clean\n",
    "print(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "\n",
    "class SpellingReplacer(object):\n",
    "    def __init__(self, dict_name = 'en_GB', max_dist = 2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_dist = 2\n",
    "\n",
    "    def replace(self, word):\n",
    "        if self.spell_dict.check(word):\n",
    "            return word\n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "\n",
    "        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
    "            return suggestions[0]\n",
    "        else:\n",
    "            return word\n",
    "def spell_check(word_list):\n",
    "    checked_list = []\n",
    "    for item in word_list:\n",
    "        replacer = SpellingReplacer()\n",
    "        r = replacer.replace(item)\n",
    "        checked_list.append(r)\n",
    "    return checked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import os\n",
    "import gensim \n",
    "from gensim import corpora, similarities, models\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import enchant\n",
    "import timeit\n",
    "import pickle \n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenizer_nopunct = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "total_tokens=[]\n",
    "os.chdir(userdir)\n",
    "count=0\n",
    "print(\"Starting to read files:\")\n",
    "\n",
    "for filepath in filepaths: \n",
    "    if count<len(filepaths)+1: \n",
    "        #print(filepath)\n",
    " \n",
    "        contents = Path(filepath).read_text()\n",
    "        #print(contents)\n",
    "        tokens_nopunct=tokenizer_nopunct.tokenize(contents)\n",
    "        concat_str=' '.join(tokens_nopunct)\n",
    "        preproc_text=tokenize(concat_str) \n",
    "         \n",
    "        #print(preproc_text)\n",
    "        #tokens=nltk.word_tokenize(prepoc_text)\n",
    "        parts = tknzr.tokenize(preproc_text)\n",
    "        \n",
    "        \n",
    "        \n",
    "        clean1=[lemmatizer.lemmatize(i.lower()) for i in parts if i not in stop]\n",
    "        clean1=[word for word in clean1 if len(word)>1]\n",
    "        \n",
    "        #clean1=[word for word in clean1 if word in english_vocab]\n",
    "        #clean1=[lemmatizer.lemmatize(word) for word in clean1 if word in glove_model]\n",
    "        #start_time = timeit.default_timer()\n",
    "        \n",
    "        clean1=spell_check(clean1)\n",
    "        \n",
    "        #time = timeit.default_timer()\n",
    "        #print(\"Spell check done in:\")\n",
    "        #print(time-start_time)        \n",
    "\n",
    " \n",
    "        \n",
    "        total_tokens.append(clean1)\n",
    "\n",
    "        if count%30==0:\n",
    "            print(str(count+1)+\" files processed.\")\n",
    "            \n",
    "\n",
    "    count=count+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.chdir('..')\n",
    "with open(\"preprocessed_user2.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(total_tokens, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
