{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for frequncy vectors\n",
    "def entropy1(freq1):\n",
    "    N=sum(freq1)\n",
    "    if N==0:\n",
    "        return float('NaN')\n",
    "    else:\n",
    "        H=0\n",
    "        for f in freq1:\n",
    "            if f!=0:\n",
    "                p=f/N\n",
    "                H=H-p*np.log2(p)\n",
    "        return H \n",
    "\n",
    "# Entropy of joint distribution from contingency tables\n",
    "def entropy2(freq):\n",
    "    N=sum(sum(freq))\n",
    "    if N==0:\n",
    "        return float('NaN')\n",
    "    else:\n",
    "        H=0\n",
    "        for frow in freq:\n",
    "            for f in frow:\n",
    "                if f!=0:\n",
    "                    p=f/N\n",
    "                    H=H-p*np.log2(p)\n",
    "        return H \n",
    "                \n",
    "# Mutual Information for contingency tables\n",
    "def mutual_info(freq): \n",
    "    freq0=np.sum(freq,axis=0)\n",
    "    freq1=np.sum(freq,axis=1)\n",
    "    return entropy1(freq0)+entropy1(freq1)-entropy2(freq)\n",
    "\n",
    "# Mutual Information for contingency tables, normalized by entropy\n",
    "def norm_mutual_info(freq): \n",
    "    freq0=np.sum(freq,axis=0)\n",
    "    freq1=np.sum(freq,axis=1)\n",
    "    return mutual_info(freq)/np.sqrt(entropy1(freq0)*entropy1(freq1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_freq_table(num_rows,num_cols,predictions,answers,label_choices):\n",
    "    freq=np.zeros((num_rows,num_cols))\n",
    "    tdict={}\n",
    "    i=0\n",
    "    for p in label_choices:\n",
    "        tdict[p]=i\n",
    "        i=i+1\n",
    "    train_size=len(predictions)\n",
    "    for i in range(train_size):\n",
    "        freq[tdict[predictions[i]]][tdict[answers[i]]]+=1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atheism\n",
      "\n",
      "\tUsing best c=1\n",
      "Model Score (mean accuracy): 0.709090909091\n",
      "\t Test: 60.99\n",
      "\t Against: 81.99 \t Favor: 40.0\n",
      "[[ 132.   13.   17.]\n",
      " [  15.   12.    3.]\n",
      " [  13.    3.   12.]]\n",
      "Normalized Mutual Information\n",
      "0.10118035106\n",
      "\n",
      "Climate Change is a Real Concern\n",
      "\n",
      "\tUsing best c=0.001\n",
      "Model Score (mean accuracy): 0.727810650888\n",
      "\t Test: 41.51\n",
      "\t Against: 0.0 \t Favor: 83.02\n",
      "[[   0.    0.    0.]\n",
      " [   1.   13.   13.]\n",
      " [  10.   22.  110.]]\n",
      "Normalized Mutual Information\n",
      "0.0655515431295\n",
      "\n",
      "Feminist Movement\n",
      "\n",
      "\tUsing best c=0.01\n",
      "Model Score (mean accuracy): 0.561403508772\n",
      "\t Test: 55.19\n",
      "\t Against: 66.07 \t Favor: 44.3\n",
      "[[ 111.   20.   22.]\n",
      " [  17.   14.    1.]\n",
      " [  55.   10.   35.]]\n",
      "Normalized Mutual Information\n",
      "0.0692518926367\n",
      "\n",
      "Hillary Clinton\n",
      "\n",
      "\tUsing best c=0.1\n",
      "Model Score (mean accuracy): 0.650847457627\n",
      "\t Test: 57.28\n",
      "\t Against: 74.02 \t Favor: 40.54\n",
      "[[ 141.   41.   27.]\n",
      " [  18.   36.    3.]\n",
      " [  13.    1.   15.]]\n",
      "Normalized Mutual Information\n",
      "0.132155100586\n",
      "\n",
      "Legalization of Abortion\n",
      "\n",
      "\tUsing best c=0.1\n",
      "Model Score (mean accuracy): 0.610714285714\n",
      "\t Test: 54.91\n",
      "\t Against: 72.18 \t Favor: 37.65\n",
      "[[ 131.   17.   26.]\n",
      " [  39.   24.    4.]\n",
      " [  19.    4.   16.]]\n",
      "Normalized Mutual Information\n",
      "0.078564657884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from happierfuntokenizing import Tokenizer\n",
    "\n",
    "\n",
    "def get_data(data_f):\n",
    "    \"\"\"\n",
    "    Load the data from file and seperate into train/tune/test sets\n",
    "\n",
    "    Output format is a dictionary that maps tweet ids to (stance, tweet) tuples.\n",
    "    Note that stance is the gold label (what is to be predicted).\n",
    "    \"\"\"\n",
    "\n",
    "    train = {}\n",
    "    test = {}\n",
    "    # Load data from file\n",
    "    # It has already been split into testing and training data\n",
    "    with open(data_f,'r',encoding='latin-1') as f:\n",
    "        for l_count, line in enumerate(f):\n",
    "            if l_count == 0:\n",
    "                continue\n",
    "            tid, _, test_train, target, stance, tweet = line.strip().split(\"\\t\")\n",
    "            if target not in train:\n",
    "                train[target] = {}\n",
    "                test[target] = {}\n",
    "            if test_train == \"test\":\n",
    "                test[target][tid] = (stance, tweet)\n",
    "            else:\n",
    "                train[target][tid] = (stance, tweet)\n",
    "\n",
    "    # Split into test/tune/train\n",
    "    new_train = {}\n",
    "    tune = {}\n",
    "    for target in train:\n",
    "        tids = train[target].keys()\n",
    "        tids1=list(train[target].keys())\n",
    "        tids1=np.array(tids1)\n",
    "        train_tids, tune_tids = train_test_split(tids1, test_size=.2)\n",
    "        new_train[target] = {tid: train[target][tid] for tid in train_tids}\n",
    "        tune[target] = {tid: train[target][tid] for tid in tune_tids}\n",
    "\n",
    "    return test, tune, new_train\n",
    "\n",
    "\n",
    "def get_char_ngram_feats(tweet, n):\n",
    "    \"\"\"Extract character ngram features\"\"\"\n",
    "    feat_template = \"CHAR_{}:\".format(n)\n",
    "    feats = []\n",
    "    for i in range(len(tweet)):\n",
    "        if i + n <= len(tweet):\n",
    "            feat = feat_template + tweet[i:i + n]\n",
    "            pair = (feat, 1)\n",
    "            if pair not in feats:\n",
    "                feats.append(pair)\n",
    "    return feats\n",
    "\n",
    "\n",
    "def get_word_ngram_feats(tokens, n):\n",
    "    \"\"\"Extract word ngram features\"\"\"\n",
    "    feat_template = \"WORD_{}:\".format(n)\n",
    "    feats = []\n",
    "    tokens=list(tokens)\n",
    "    for i in range(len(tokens)):\n",
    "        if i + n <= len(tokens):\n",
    "            feat = feat_template + \"|\".join(tokens[i:i + n])\n",
    "            pair = (feat, 1)\n",
    "            if pair not in feats:\n",
    "                feats.append(pair)\n",
    "    return feats\n",
    "\n",
    "\n",
    "def create_feat_dict(data, threshold=0):\n",
    "    \"\"\"\n",
    "    Create a dictionary that maps feature names to indices (what column of the\n",
    "    feature matrix they are).\n",
    "\n",
    "    Can optionally specify a threshold; only features which occur more than this\n",
    "    threshold will be included.\n",
    "    \"\"\"\n",
    "    feat_counts = {}\n",
    "    tokenizer = Tokenizer()\n",
    "    for tid in data:\n",
    "        tweet = data[tid][1]\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "        feats = []\n",
    "        feats += get_char_ngram_feats(tweet, 2)\n",
    "        feats += get_char_ngram_feats(tweet, 3)\n",
    "        feats += get_char_ngram_feats(tweet, 4)\n",
    "        feats += get_char_ngram_feats(tweet, 5)\n",
    "        feats += get_word_ngram_feats(tokens, 1)\n",
    "        feats += get_word_ngram_feats(tokens, 2)\n",
    "        feats += get_word_ngram_feats(tokens, 3)\n",
    "\n",
    "        for feat_name, _ in feats:\n",
    "            if feat_name not in feat_counts:\n",
    "                feat_counts[feat_name] = 0\n",
    "            feat_counts[feat_name] += 1\n",
    "    # Only keep features which occur more often than the given threshold\n",
    "    feat_dict = {}\n",
    "    for feat_name in feat_counts:\n",
    "        if feat_counts[feat_name] > threshold:\n",
    "            feat_dict[feat_name] = len(feat_dict)\n",
    "    return feat_dict\n",
    "\n",
    "\n",
    "def create_matrix(data, feat_dict, ncols=None):\n",
    "    \"\"\"\n",
    "    Given the raw data and the dictionary of feature names, creates the feature\n",
    "    matrix which will be used for training/testing.\n",
    "\n",
    "    ncols is used to specify the number of cols (features) that the matrix will have.\n",
    "    This is necessary because the test/train/tune matrices all need the same number.\n",
    "    \"\"\"\n",
    "    X = [[], [], []]\n",
    "    Y = []\n",
    "    tokenizer = Tokenizer()\n",
    "    cur_row = 0\n",
    "    for tid in data:\n",
    "        stance, tweet = data[tid]\n",
    "\n",
    "        if stance == \"AGAINST\":\n",
    "            Y.append(-1)\n",
    "        elif stance == \"FAVOR\":\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            Y.append(0)\n",
    "\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "        feats = []\n",
    "        feats += get_char_ngram_feats(tweet, 2)\n",
    "        feats += get_char_ngram_feats(tweet, 3)\n",
    "        feats += get_char_ngram_feats(tweet, 4)\n",
    "        feats += get_char_ngram_feats(tweet, 5)\n",
    "        feats += get_word_ngram_feats(tokens, 1)\n",
    "        feats += get_word_ngram_feats(tokens, 2)\n",
    "        feats += get_word_ngram_feats(tokens, 3)\n",
    "\n",
    "        for feat_name, feat_val in feats:\n",
    "            if feat_name not in feat_dict:\n",
    "                continue\n",
    "            feat_idx = feat_dict[feat_name]\n",
    "            X[0].append(feat_val)\n",
    "            X[1].append(cur_row)\n",
    "            X[2].append(feat_idx)\n",
    "        cur_row += 1\n",
    "    if ncols is not None:\n",
    "        nrows = len(Y)\n",
    "        X_mat = coo_matrix((X[0], (X[1], X[2])), shape=(nrows, ncols))\n",
    "    else:\n",
    "        X_mat = coo_matrix((X[0], (X[1], X[2])))\n",
    "    X_array = X_mat.toarray()\n",
    "    Y_array = np.array(Y)\n",
    "    return X_array, Y_array\n",
    "\n",
    "\n",
    "def compute_score(predictions, labels):\n",
    "    \"\"\"\n",
    "    Compute the F1 score for the model's predictions\n",
    "    \"\"\"\n",
    "    # true positive, num guessed, num gold\n",
    "    against = [0.0, 0.0, 0.0]\n",
    "    favor = [0.0, 0.0, 0.0]\n",
    "    for i in range(len(predictions)):\n",
    "        predict = predictions[i]\n",
    "        label = labels[i]\n",
    "        if predict == label:\n",
    "            if label == -1:\n",
    "                against[0] += 1\n",
    "            elif label == 1:\n",
    "                favor[0] += 1\n",
    "        if predict == 1:\n",
    "            favor[1] += 1\n",
    "        elif predict == -1:\n",
    "            against[1] += 1\n",
    "        if label == 1:\n",
    "            favor[2] += 1\n",
    "        elif label == -1:\n",
    "            against[2] += 1\n",
    "    scores = []\n",
    "    for cat in [against, favor]:\n",
    "        prec = 0.0\n",
    "        rec = 0.0\n",
    "        f1 = 0.0\n",
    "        if cat[1] > 0:\n",
    "            prec = cat[0] / cat[1]\n",
    "        if cat[2] > 0:\n",
    "            rec = cat[0] / cat[2]\n",
    "        if prec + rec > 0:\n",
    "            f1 = 2 * prec * rec / (prec + rec)\n",
    "        scores.append((cat[0], cat[1], cat[2], prec, rec, f1))\n",
    "    return scores\n",
    "\n",
    "\n",
    "def train_model(train_X, train_Y, tune_X, tune_Y):\n",
    "    # SVM parameters\n",
    "    fit_intercept = True\n",
    "    penalty = \"l2\"\n",
    "    loss = \"squared_hinge\"\n",
    "    dual = True\n",
    "    tol = .0001\n",
    "    max_iter = 1000\n",
    "    random_state = 5\n",
    "\n",
    "    c_cands = [10 ** i for i in range(-5, 1, 1)]\n",
    "    c_votes = {c: 0 for c in c_cands}\n",
    "\n",
    "    # Use the tune set to choose the best value for c\n",
    "    scores = []\n",
    "    for c in c_cands:\n",
    "        classif = LinearSVC(C=c, random_state=random_state, penalty=penalty,\n",
    "                            fit_intercept=fit_intercept, loss=loss, dual=dual,\n",
    "                            tol=tol, max_iter=max_iter)\n",
    "        classif.fit(train_X, train_Y)\n",
    "        predictions = classif.predict(tune_X)\n",
    "        against, favor = compute_score(predictions, tune_Y)\n",
    "        score = (against[-1] + favor[-1]) / 2\n",
    "        scores.append(score)\n",
    "    best_score = max(scores)\n",
    "    best_cs = []\n",
    "    for i in range(len(scores)):\n",
    "        if scores[i] == best_score:\n",
    "            best_cs.append(str(c_cands[i]))\n",
    "            c_votes[c_cands[i]] += 1\n",
    "\n",
    "    votes = [(y, x) for x, y in c_votes.items()]\n",
    "    c_opt = sorted(votes, reverse=True)\n",
    "\n",
    "    # Once best value for c is found, train model with that value\n",
    "    print(\"\\tUsing best c={}\".format(c_opt[0][1]))\n",
    "    classif = LinearSVC(C=c_opt[0][1], random_state=random_state, penalty=penalty,\n",
    "                        fit_intercept=fit_intercept, loss=loss, dual=dual, tol=tol,\n",
    "                        max_iter=max_iter)\n",
    "    classif.fit(train_X, train_Y)\n",
    "    return classif\n",
    "\n",
    "\n",
    "def main():\n",
    "    threshold = 0\n",
    "    data_f = \"../stance/data/dataset.tsv\"\n",
    "\n",
    "    test, tune, train = get_data(data_f)\n",
    "\n",
    "    targets = [\"Atheism\", \"Climate Change is a Real Concern\", \"Feminist Movement\",\n",
    "               \"Hillary Clinton\", \"Legalization of Abortion\"]\n",
    "    stances=['AGAINST','NONE','FAVOR']\n",
    "\n",
    "    target_num=len(targets)\n",
    "    stance_num=len(stances)\n",
    "    for target in targets:\n",
    "        print(target+'\\n')\n",
    "        feat_dict = create_feat_dict(train[target], threshold=threshold)\n",
    "        num_feats = len(feat_dict)\n",
    "        train_X, train_Y = create_matrix(train[target], feat_dict, ncols=num_feats)\n",
    "        tune_X, tune_Y = create_matrix(tune[target], feat_dict, ncols=num_feats)\n",
    "        test_X, test_Y = create_matrix(test[target], feat_dict, ncols=num_feats)\n",
    "\n",
    "        model = train_model(train_X, train_Y, tune_X, tune_Y)\n",
    "        predictions = model.predict(test_X)\n",
    "        against, favor = compute_score(predictions, test_Y)\n",
    "        score = (against[-1] + favor[-1]) / 2 * 100\n",
    "        print('Model Score (mean accuracy): '+str(model.score(test_X,test_Y)))\n",
    "        print(\"\\t\", \"Test:\", round(score, 2))\n",
    "        print(\"\\t\", \"Against:\",round(against[-1]*100,2),\"\\t\", \"Favor:\",round(favor[-1]*100,2))\n",
    "        pred_string=[]\n",
    "        for pred in predictions: \n",
    "            if pred==-1:\n",
    "                pred_string.append(\"AGAINST\")\n",
    "            elif pred==0:\n",
    "                pred_string.append(\"NONE\")\n",
    "            else:\n",
    "                pred_string.append(\"FAVOR\")\n",
    "        testY_string=[]\n",
    "        for t in test_Y: \n",
    "            if t==-1:\n",
    "                testY_string.append(\"AGAINST\")\n",
    "            elif t==0:\n",
    "                testY_string.append(\"NONE\")\n",
    "            else:\n",
    "                testY_string.append(\"FAVOR\")\n",
    "        freqS=create_freq_table(stance_num,stance_num,pred_string,testY_string,stances)\n",
    "        print(freqS)\n",
    "        print('Normalized Mutual Information')\n",
    "        print(norm_mutual_info(freqS))\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiclassScore(answers,pred):\n",
    "    favord=0\n",
    "    favorn=0\n",
    "    againstd=0\n",
    "    againstn=0\n",
    "    noned=0\n",
    "    nonen=0\n",
    "    wrong_indices=[]\n",
    "    for i in range (0,len(answers)):\n",
    "        if answers[i]==\"AGAINST\": \n",
    "            againstd+=1\n",
    "        if answers[i]==\"FAVOR\": \n",
    "            favord+=1 \n",
    "        if answers[i]==\"NONE\": \n",
    "            noned+=1\n",
    "        if answers[i]==pred[i]: \n",
    "            if answers[i]==\"AGAINST\": \n",
    "              \n",
    "                againstn+=1\n",
    "            if answers[i]==\"FAVOR\": \n",
    "              \n",
    "                favorn+=1\n",
    "            if answers[i]==\"NONE\": \n",
    "             \n",
    "                nonen+=1\n",
    "        if answers[i]!=pred[i]: \n",
    "            wrong_indices.append(i)\n",
    "       \n",
    "    if favord==0: \n",
    "        favord=1\n",
    "    if againstd==0: \n",
    "        againstd=1\n",
    "    if noned==0: \n",
    "        noned=1\n",
    "    print('\\t'+'against'+'\\n')\n",
    "    print('\\t'+'\\t'+str(againstn/againstd))\n",
    "    print('\\t'+'favor'+'\\n')\n",
    "    print('\\t'+'\\t'+str(favorn/favord))\n",
    "    print('\\t'+'none'+'\\n')\n",
    "    print('\\t'+'\\t'+str(nonen/noned))\n",
    "    return wrong_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atheism\n",
      "\n",
      "\tBernoulli Score\n",
      "\n",
      "\t0.727272727273\n",
      "\n",
      "\tagainst\n",
      "\n",
      "\t\t1.0\n",
      "\tfavor\n",
      "\n",
      "\t\t0.0\n",
      "\tnone\n",
      "\n",
      "\t\t0.0\n",
      "Contingency Table (Rows Representing predicted stances, columns representing correct stances)\n",
      "\n",
      "[[ 160.   28.   32.]\n",
      " [   0.    0.    0.]\n",
      " [   0.    0.    0.]]\n",
      "Normalized Mutual Information\n",
      "nan\n",
      "\n",
      "\tMultinomial Score\n",
      "\n",
      "\t0.740909090909\n",
      "\n",
      "\tagainst\n",
      "\n",
      "\t\t1.0\n",
      "\tfavor\n",
      "\n",
      "\t\t0.0625\n",
      "\tnone\n",
      "\n",
      "\t\t0.03571428571428571\n",
      "Contingency Table (Rows Representing predicted stances, columns representing correct stances)\n",
      "\n",
      "[[ 160.   28.   32.]\n",
      " [   0.    0.    0.]\n",
      " [   0.    0.    0.]]\n",
      "Normalized Mutual Information\n",
      "nan\n",
      "\n",
      "Feminist Movement\n",
      "\n",
      "\tBernoulli Score\n",
      "\n",
      "\t0.638596491228\n",
      "\n",
      "\tagainst\n",
      "\n",
      "\t\t0.9726775956284153\n",
      "\tfavor\n",
      "\n",
      "\t\t0.06896551724137931\n",
      "\tnone\n",
      "\n",
      "\t\t0.0\n",
      "Contingency Table (Rows Representing predicted stances, columns representing correct stances)\n",
      "\n",
      "[[ 178.   44.   54.]\n",
      " [   0.    0.    0.]\n",
      " [   5.    0.    4.]]\n",
      "Normalized Mutual Information\n",
      "0.024412451723\n",
      "\n",
      "\tMultinomial Score\n",
      "\n",
      "\t0.610526315789\n",
      "\n",
      "\tagainst\n",
      "\n",
      "\t\t0.8961748633879781\n",
      "\tfavor\n",
      "\n",
      "\t\t0.1724137931034483\n",
      "\tnone\n",
      "\n",
      "\t\t0.0\n",
      "Contingency Table (Rows Representing predicted stances, columns representing correct stances)\n",
      "\n",
      "[[ 178.   44.   54.]\n",
      " [   0.    0.    0.]\n",
      " [   5.    0.    4.]]\n",
      "Normalized Mutual Information\n",
      "0.024412451723\n",
      "\n",
      "Climate Change is a Real Concern\n",
      "\n",
      "\tBernoulli Score\n",
      "\n",
      "\t0.733727810651\n",
      "\n",
      "\tagainst\n",
      "\n",
      "\t\t0.0\n",
      "\tfavor\n",
      "\n",
      "\t\t0.967479674796748\n",
      "\tnone\n",
      "\n",
      "\t\t0.14285714285714285\n",
      "Contingency Table (Rows Representing predicted stances, columns representing correct stances)\n",
      "\n",
      "[[   0.    0.    0.]\n",
      " [   0.    5.    4.]\n",
      " [  11.   30.  119.]]\n",
      "Normalized Mutual Information\n",
      "0.0477906038949\n",
      "\n",
      "\tMultinomial Score\n",
      "\n",
      "\t0.733727810651\n",
      "\n",
      "\tagainst\n",
      "\n",
      "\t\t0.0\n",
      "\tfavor\n",
      "\n",
      "\t\t0.959349593495935\n",
      "\tnone\n",
      "\n",
      "\t\t0.17142857142857143\n",
      "Contingency Table (Rows Representing predicted stances, columns representing correct stances)\n",
      "\n",
      "[[   0.    0.    0.]\n",
      " [   0.    5.    4.]\n",
      " [  11.   30.  119.]]\n",
      "Normalized Mutual Information\n",
      "0.0477906038949\n",
      "\n",
      "Hillary Clinton\n",
      "\n",
      "\tBernoulli Score\n",
      "\n",
      "\t0.583050847458\n",
      "\n",
      "\tagainst\n",
      "\n",
      "\t\t1.0\n",
      "\tfavor\n",
      "\n",
      "\t\t0.0\n",
      "\tnone\n",
      "\n",
      "\t\t0.0\n",
      "Contingency Table (Rows Representing predicted stances, columns representing correct stances)\n",
      "\n",
      "[[ 172.   78.   45.]\n",
      " [   0.    0.    0.]\n",
      " [   0.    0.    0.]]\n",
      "Normalized Mutual Information\n",
      "nan\n",
      "\n",
      "\tMultinomial Score\n",
      "\n",
      "\t0.586440677966\n",
      "\n",
      "\tagainst\n",
      "\n",
      "\t\t0.9941860465116279\n",
      "\tfavor\n",
      "\n",
      "\t\t0.0\n",
      "\tnone\n",
      "\n",
      "\t\t0.02564102564102564\n",
      "Contingency Table (Rows Representing predicted stances, columns representing correct stances)\n",
      "\n",
      "[[ 172.   78.   45.]\n",
      " [   0.    0.    0.]\n",
      " [   0.    0.    0.]]\n",
      "Normalized Mutual Information\n",
      "nan\n",
      "\n",
      "Legalization of Abortion\n",
      "\n",
      "\tBernoulli Score\n",
      "\n",
      "\t0.678571428571\n",
      "\n",
      "\tagainst\n",
      "\n",
      "\t\t1.0\n",
      "\tfavor\n",
      "\n",
      "\t\t0.0\n",
      "\tnone\n",
      "\n",
      "\t\t0.022222222222222223\n",
      "Contingency Table (Rows Representing predicted stances, columns representing correct stances)\n",
      "\n",
      "[[ 189.   44.   46.]\n",
      " [   0.    1.    0.]\n",
      " [   0.    0.    0.]]\n",
      "Normalized Mutual Information\n",
      "0.0460899664043\n",
      "\n",
      "\tMultinomial Score\n",
      "\n",
      "\t0.678571428571\n",
      "\n",
      "\tagainst\n",
      "\n",
      "\t\t0.9841269841269841\n",
      "\tfavor\n",
      "\n",
      "\t\t0.0\n",
      "\tnone\n",
      "\n",
      "\t\t0.08888888888888889\n",
      "Contingency Table (Rows Representing predicted stances, columns representing correct stances)\n",
      "\n",
      "[[ 189.   44.   46.]\n",
      " [   0.    1.    0.]\n",
      " [   0.    0.    0.]]\n",
      "Normalized Mutual Information\n",
      "0.0460899664043\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sikata/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:38: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB\n",
    "def get_data(data_f):\n",
    "    \"\"\"\n",
    "    Load the data from file and seperate into train/tune/test sets\n",
    "\n",
    "    Output format is a dictionary that maps tweet ids to (stance, tweet) tuples.\n",
    "    Note that stance is the gold label (what is to be predicted).\n",
    "    \"\"\"\n",
    "\n",
    "    train = {}\n",
    "    test = {}\n",
    "    user_list=[]\n",
    "    # Load data from file\n",
    "    # It has already been split into testing and training data\n",
    "    with open(data_f,'r',encoding='latin-1') as f:\n",
    "        for l_count, line in enumerate(f):\n",
    "            if l_count == 0:\n",
    "                continue\n",
    "            tid, user, test_train, target, stance, tweet = line.strip().split(\"\\t\")\n",
    "            if user not in user_list:\n",
    "                user_list.append(user)\n",
    "            if target not in train:\n",
    "                train[target] = {}\n",
    "                test[target] = {}\n",
    "            if test_train == \"test\":\n",
    "                test[target][tid] = (stance, tweet)\n",
    "            else:\n",
    "                train[target][tid] = (stance, tweet)\n",
    "\n",
    "    # Split into test/tune/train\n",
    "    new_train = {}\n",
    "    for target in train:\n",
    "        tids = train[target].keys()\n",
    "        tids1=list(train[target].keys())\n",
    "    return test,train,user_list\n",
    "\n",
    "data_f='../stance/datacopy.tsv'\n",
    "users=[]\n",
    "testset,trainset,users=get_data(data_f)\n",
    "\n",
    "        \n",
    "targets=[\"Atheism\", \"Feminist Movement\",\"Climate Change is a Real Concern\",\"Hillary Clinton\",\"Legalization of Abortion\"]\n",
    "stances=['AGAINST','NONE','FAVOR']\n",
    "\n",
    "target_num=len(targets)\n",
    "stance_num=len(stances)\n",
    "tweets_of_target={}\n",
    "stances_of_target={}\n",
    "choices=['train','test']\n",
    "for opt in choices:\n",
    "    tweets_of_target[opt]={}\n",
    "    stances_of_target[opt]={}\n",
    "    for target in targets: \n",
    "        tweets_of_target[opt][target]=[]\n",
    "        stances_of_target[opt][target]=[]\n",
    "\n",
    "for target in targets:  \n",
    "    choice=''\n",
    "    for tid in trainset[target]: \n",
    "        choice='train'\n",
    "        tweets_of_target[choice][target].append(trainset[target][tid][1])\n",
    "        stances_of_target[choice][target].append(trainset[target][tid][0])\n",
    "     \n",
    "    for tid in testset[target]: \n",
    "        choice='test'\n",
    "        tweets_of_target[choice][target].append(testset[target][tid][1])\n",
    "        stances_of_target[choice][target].append(testset[target][tid][0])\n",
    "       \n",
    "        \n",
    "    vectorizer=TfidfVectorizer(binary=True,sublinear_tf=True,max_df=0.5, analyzer='word', \n",
    "                 stop_words='english')\n",
    "    \n",
    "    train_matrix=vectorizer.fit_transform(tweets_of_target['train'][target])\n",
    "    test_matrix=vectorizer.transform(tweets_of_target['test'][target])\n",
    "    model=BernoulliNB()\n",
    "    model.fit(train_matrix,stances_of_target['train'][target])\n",
    "    predictions=model.predict(test_matrix)\n",
    "    print(target+'\\n')\n",
    "    print('\\t'+\"Bernoulli Score\"+'\\n')\n",
    "    print('\\t'+str(model.score(test_matrix,stances_of_target['test'][target]))+'\\n')\n",
    "    multiclassScore(stances_of_target['test'][target],predictions)\n",
    "\n",
    "    freqB=create_freq_table(stance_num,stance_num,predictions,stances_of_target['test'][target],stances)\n",
    "    \n",
    "    print('Contingency Table (Rows Representing predicted stances, columns representing correct stances)'+'\\n')\n",
    "    print(freqB)\n",
    "    print('Normalized Mutual Information')\n",
    "    print(norm_mutual_info(freqB))\n",
    "#precision_recall_fscore_support(stances_test,predictions, labels=['FAVOR','NONE','AGAINST'])\n",
    "    print(\"\")\n",
    "\n",
    "    multi=MultinomialNB()\n",
    "    multi.fit(train_matrix,stances_of_target['train'][target])\n",
    "    predictions1=multi.predict(test_matrix)\n",
    "    print('\\t'+\"Multinomial Score\"+'\\n')\n",
    "    print('\\t'+str(multi.score(test_matrix,stances_of_target['test'][target]))+'\\n')\n",
    "    multiclassScore(stances_of_target['test'][target],predictions1)\n",
    "#precision_recall_fscore_support(stances_test,predictions1)\n",
    "    freqM=create_freq_table(stance_num,stance_num,predictions,stances_of_target['test'][target],stances)\n",
    "    print('Contingency Table (Rows Representing predicted stances, columns representing correct stances)'+'\\n')\n",
    "    print(freqM)\n",
    "   \n",
    "    print('Normalized Mutual Information')\n",
    "    print(norm_mutual_info(freqM))\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
